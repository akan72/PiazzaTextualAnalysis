{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.39)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jieba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-cf9538aa8666>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jieba'"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import nltk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "from sklearn.decomposition import LatentDirichletAllocation \n",
    "from gensim import corpora, models, similarities\n",
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Posts & post titles from df --> lists of strings\n",
    "#Each string in list is the full text of the post or title, will be parsed later\n",
    "\n",
    "dfs = glob.glob('data/dataframes/*.p')\n",
    "frames = []\n",
    "for cdf in dfs:\n",
    "    frames.append(pd.read_pickle(cdf))\n",
    "df = pd.concat(frames)\n",
    "df = df[df.is_student == True]\n",
    "dataList = df[\"text\"].tolist()\n",
    "subjectList = df[\"subject\"].tolist()\n",
    "z = list(zip(dataList, subjectList))\n",
    "\n",
    "#Randomizing order if later want to split into training and test inputs\n",
    "# random.shuffle(z)\n",
    "# dataList[:], subjectList[:] = zip(*z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://medium.com/better-programming/introduction-to-gensim-calculating-text-similarity-9e8b55de342d\n",
    "\n",
    "texts = dataList\n",
    "texts = [jieba.lcut(text) for text in texts] #tokenize text of post, turns string into list of substrings\n",
    "dictionary = corpora.Dictionary(texts) #make dictionary where postId --> list of substrings\n",
    "feature_cnt = len(dictionary.token2id) #get doc count for later\n",
    "corpus = [dictionary.doc2bow(text) for text in texts] #corpus is bag of words from docs in dictionary\n",
    "tfidf = models.TfidfModel(corpus) #construct tf-idf model on corpus\n",
    "\n",
    "#repeat the above but for post titles\n",
    "subjects = subjectList\n",
    "subjects = [jieba.lcut(subject) for subject in subjects]\n",
    "subDict = corpora.Dictionary(subjects)\n",
    "sub_fc = len(subDict.token2id)\n",
    "subCorpus = [subDict.doc2bow(subject) for subject in subjects]\n",
    "subTfidf = models.TfidfModel(subCorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### If you want to input in the command line\n",
    "# print(\"Title:\")\n",
    "# subKw = input()\n",
    "# print(\"Post:\")\n",
    "# keyword = input()\n",
    "#####\n",
    "\n",
    "### Sample inputs for demo\n",
    "keyword = \"I used the instructions on Sakai for adding the junit tests but I'm still getting an error. Does anyone know how to fix this?\"\n",
    "subKw = \"Error adding Junit tests\"\n",
    "# keyword = \"Is there going to be an curve on the midterm or are our gradescope scores the final grade?\"\n",
    "# subKw = \"Exam grades\"\n",
    "# keyword = \"Will the questions on the final be similar to the types of questions on the midterm?\"\n",
    "# subKw = \"Final exam format\"\n",
    "# keyword = \"For ShortButFairDispatcher, I'm not sure what it's asking. How are we supposed to determine which driver is the closest if the closest driver was in the last 5?\"\n",
    "# subKw = \"ShortestButFairDispatcher\"\n",
    "#####\n",
    "\n",
    "\n",
    "#source: https://medium.com/better-programming/introduction-to-gensim-calculating-text-similarity-9e8b55de342d\n",
    "kw_vector = dictionary.doc2bow(jieba.lcut(keyword)) #cuts input phrase into sparse vector\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features = feature_cnt) #calculates vector similarity\n",
    "sim = index[tfidf[kw_vector]] #gets similarity score of keyword vector to each item in corpus\n",
    "\n",
    "#repeats above block for post titles\n",
    "subKw_vector = subDict.doc2bow(jieba.lcut(subKw))\n",
    "subIndex = similarities.SparseMatrixSimilarity(subTfidf[subCorpus], num_features = sub_fc)\n",
    "subSim = subIndex[subTfidf[subKw_vector]]     \n",
    "\n",
    "#prints out similar posts from each of 3 formulas\n",
    "print(\"These posts might be similar to what you're looking for: \\n\")\n",
    "\n",
    "for i in range(len(sim)):\n",
    "    if sim[i]*1+subSim[i]*0.5 > balanced: #\n",
    "        balanced = sim[i]*1+subSim[i]*0.5\n",
    "        balanced_id = i\n",
    "    if sim[i]*1+subSim[i]*1 > sub_bias:\n",
    "        sub_bias = sim[i]*1+subSim[i]*1\n",
    "        sub_bias_id = i   \n",
    "    if sim[i]*1+subSim[i]*0.1 > post_bias:\n",
    "        post_bias = sim[i]*1+subSim[i]*0.1\n",
    "        post_bias_id = i  \n",
    "\n",
    "print('keyword is similar to text%d: %.2f' % (balanced_id, balanced))\n",
    "print(dataList[balanced_id])\n",
    "print(\"\\n\")\n",
    "    \n",
    "\n",
    "print('keyword is similar to text%d: %.2f' % (sub_bias_id, sub_bias))\n",
    "print(dataList[sub_bias_id])\n",
    "print(\"\\n\")\n",
    "      \n",
    "        \n",
    "print('keyword is similar to text%d: %.2f' % (post_bias_id, post_bias))\n",
    "print(dataList[post_bias_id])\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
